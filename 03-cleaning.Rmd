# Data transformation

```{r, message=FALSE, warning=FALSE}
library(here)
library(readr)
library(tidyverse)
library(socviz)
library(lubridate)
```

There are two main problems when dealing with our data:

- *Timestamp* column is in undesirable format
- Original dataset is too big to deal with

## Transforming Timestamp

First, we mutate a new column *Time* by using as_datatime. Given our data took place in China, we shall set timezone at Asia/Shanghai;
```{r eval = FALSE, echo=FALSE}
data=read.csv('UserBehavior.csv') # 100150806 rows,5 columns
data <- na.omit(data) # no missing value
colnames(data) <-c('User_ID','Item_ID','Category_ID','Behavior_type','Timestamp')
data<- data %>% mutate(Time=as_datetime(Timestamp,tz='Asia/Shanghai'))
```

Second, by using the combination of *as.numeric* and *as.POSIXct*, we can clearly define the *Timestamp* value of the boundaries for each day.

Third, we remove those values which lay out of our boundaries for *2017-11-26* and *2017-12-2*. There are 55576 such values in total.

Lastly, we use the Timestamp boundaries to mutate a new column to record days of the week, and this will be useful for us to splitting our data into parts later.

```{r eval = FALSE, echo=FALSE}
start = as.numeric(as.POSIXct("2017-11-26 00:00:00",tz="Asia/Shanghai"))
end = as.numeric(as.POSIXct("2017-12-2 24:00:00",tz="Asia/Shanghai"))
min =  as.numeric(as.POSIXct("2017-11-25 00:00:00",tz="Asia/Shanghai"))
max =  as.numeric(as.POSIXct("2017-12-3 24:00:00",tz="Asia/Shanghai"))
daystart = c(as.POSIXct("2017-11-26 00:00:00",tz="Asia/Shanghai"),
             as.POSIXct("2017-11-27 00:00:00",tz="Asia/Shanghai"),
             as.POSIXct("2017-11-28 00:00:00",tz="Asia/Shanghai"),
             as.POSIXct("2017-11-29 00:00:00",tz="Asia/Shanghai"),
             as.POSIXct("2017-11-30 00:00:00",tz="Asia/Shanghai"),
             as.POSIXct("2017-12-1 00:00:00",tz="Asia/Shanghai"),
             as.POSIXct("2017-12-2 00:00:00",tz="Asia/Shanghai"))

one_week_data<-data %>% filter(Timestamp>=start & Timestamp<=end) # 76013286 rows
special_data_Sat<-data %>% filter(Timestamp<start & Timestamp>min) # 10419855 rows
special_data_Sun<-data %>% filter(Timestamp>end & Timestamp<max) # 13661930 rows
data<-filter(data,Timestamp>=min & Timestamp<=max)
sprintf('The number of False Timestamp is: %d ',(100150806-nrow(data)))
one_week_data<- one_week_data %>% mutate(day= ifelse(Time<daystart[2],'Mon',ifelse(Time<daystart[3],'Tue',ifelse(Time<daystart[4],'Wed',ifelse(Time<daystart[5],'Thur',ifelse(Time<daystart[6],'Fri',ifelse(Time<daystart[7],'Sat','Sun')))))))
```

## Splitting datasets

There are three ways to splitting our datasets to make it easier to process:

- Splitting according to week of days, so we focus merely on the fluctuation on a certain day. 
- Splitting according to Behavior type. In fact, the clicking behaviors shall occupy 9/10 of data, we can ignore it if we do not focus on the behavior pattern of customers.
- Splitting according to User_ID. There are about 1 million users in the dataset and it's actually unnecessary for us to just observe trend and make visualization. we can focus on the active buyers, or just randomly select them.

```{r eval = FALSE, echo=FALSE}

no_pv_data<-filter(data,Behavior_type!='pv')
write.csv(no_pv_data,file = "no_pv_data.csv")
write.csv(special_data_Sat,file = "2017-11-25.csv")
write.csv(special_data_Sun,file = "2017-12-03.csv")
write.csv(filter(one_week_data,day=='Sun'),file = "2017-11-26.csv")
write.csv(filter(one_week_data,day=='Mon'),file = "2017-11-27.csv")
write.csv(filter(one_week_data,day=='Tue'),file = "2017-11-28.csv")
write.csv(filter(one_week_data,day=='Wed'),file = "2017-11-29.csv")
write.csv(filter(one_week_data,day=='Thur'),file = "2017-11-30.csv")
write.csv(filter(one_week_data,day=='Fri'),file = "2017-12-01.csv")
write.csv(filter(one_week_data,day=='Sat'),file = "2017-12-02.csv")
```